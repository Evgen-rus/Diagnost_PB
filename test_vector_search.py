#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ð¹ ÑÐºÑ€Ð¸Ð¿Ñ‚ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°

ÐŸÐ¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¿Ñ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ð¾Ð¸ÑÐº Ð¿Ð¾ Ð±Ð°Ð·Ðµ Ð·Ð½Ð°Ð½Ð¸Ð¹ Ð±ÐµÐ· Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð±Ð¾Ñ‚Ð°
"""

import os
import sqlite3
import sys
from dotenv import load_dotenv
from utils.vector_search import FAISSVectorStore, get_context_for_query, search_relevant_chunks
from config import EMBEDDING_DIMENSION

# Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ
load_dotenv()

def test_vector_search():
    """
    Ð¢ÐµÑÑ‚Ð¸Ñ€ÑƒÐµÑ‚ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð¿Ð¾ Ð±Ð°Ð·Ðµ Ð·Ð½Ð°Ð½Ð¸Ð¹
    """
    print("ðŸ” Ð¢Ð•Ð¡Ð¢ Ð’Ð•ÐšÐ¢ÐžÐ ÐÐžÐ“Ðž ÐŸÐžÐ˜Ð¡ÐšÐ")
    print("=" * 60)
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ API ÐºÐ»ÑŽÑ‡
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ API ÐºÐ»ÑŽÑ‡ OpenAI Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½!")
        print("ðŸ’¡ Ð”Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ OPENAI_API_KEY Ð² Ñ„Ð°Ð¹Ð» .env")
        return
    
    # ÐŸÑƒÑ‚ÑŒ Ðº Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…
    db_path = os.path.join(os.getcwd(), 'knowledge_base_v2.db')
    
    if not os.path.exists(db_path):
        print(f"âŒ Ð‘Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°: {db_path}")
        print("ðŸ’¡ Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ: python load_excel_to_vectordb.py")
        return
    
    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ FAISS Ð¸Ð½Ð´ÐµÐºÑ
    try:
        vector_store = FAISSVectorStore(embedding_dimension=EMBEDDING_DIMENSION)
        if not vector_store.load_index():
            print("âŒ FAISS Ð¸Ð½Ð´ÐµÐºÑ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½!")
            print("ðŸ’¡ Ð—Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ: python build_faiss_index.py")
            return
        print("âœ… Ð’ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ð¹ Ð¸Ð½Ð´ÐµÐºÑ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½")
    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¸Ð½Ð´ÐµÐºÑÐ°: {e}")
        return
    
    # ÐŸÐ¾Ð´ÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ÑÑ Ðº Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ñ‡Ð°Ð½ÐºÐ¾Ð²
        cursor.execute("SELECT COUNT(*) FROM chunks")
        chunk_count = cursor.fetchone()[0]
        print(f"ðŸ“Š ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ {chunk_count} Ñ‡Ð°Ð½ÐºÐ¾Ð² Ð² Ð±Ð°Ð·Ðµ Ð´Ð°Ð½Ð½Ñ‹Ñ…")
        
        if chunk_count == 0:
            print("âŒ Ð‘Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¿ÑƒÑÑ‚Ð°!")
            return
        
    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ñ Ðº Ð±Ð°Ð·Ðµ: {e}")
        return
    
    # Ð¢ÐµÑÑ‚Ð¾Ð²Ñ‹Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹
    test_queries = [
        "Ð§Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ Ð´Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ°?",
        "ÐšÐ°Ðº Ð¿Ñ€Ð¾Ð²Ð¾Ð´Ð¸Ñ‚ÑŒ Ð°Ð½Ð°Ð»Ð¸Ð·?",
        "ÐœÐµÑ‚Ð¾Ð´Ñ‹ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ",
        "ÐšÐ»Ð¸Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸",
        "Ð›ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ð°Ñ†Ð¸ÐµÐ½Ñ‚Ð¾Ð²"
    ]
    
    print("\nðŸŽ¯ Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð« ÐŸÐžÐ˜Ð¡ÐšÐ:")
    print("=" * 60)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{i}. Ð—Ð°Ð¿Ñ€Ð¾Ñ: '{query}'")
        print("-" * 40)
        
        try:
            # ÐŸÐ¾Ð¸ÑÐº Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð²
            relevant_chunks = search_relevant_chunks(query, vector_store, conn)
            
            if not relevant_chunks:
                print("âŒ ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²")
                continue
            
            # ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹
            for j, chunk in enumerate(relevant_chunks, 1):
                # Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¹ Ñ€Ð°ÑÑ‡ÐµÑ‚ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚Ð¸: Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¾Ð±Ñ€Ð°Ñ‚Ð½ÑƒÑŽ Ñ„ÑƒÐ½ÐºÑ†Ð¸ÑŽ
                # Ñ‡ÐµÐ¼ Ð¼ÐµÐ½ÑŒÑˆÐµ Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ðµ - Ñ‚ÐµÐ¼ Ð²Ñ‹ÑˆÐµ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚ÑŒ (0.0-1.0)
                distance = chunk.get('distance', float('inf'))
                if distance == 0:
                    relevance = 1.0
                else:
                    relevance = round(1.0 / (1.0 + distance), 4)
                
                doc_id = chunk.get('document_id', 'N/A')
                doc_type = chunk.get('doc_type_short', 'N/A')
                text_preview = chunk.get('text', chunk.get('content', ''))[:100] + "..."
                
                print(f"   {j}. Ð ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð¾ÑÑ‚ÑŒ: {relevance:.3f} (Ñ€Ð°ÑÑÑ‚Ð¾ÑÐ½Ð¸Ðµ: {distance:.3f})")
                print(f"      Ð”Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚: {doc_id} ({doc_type})")
                print(f"      Ð¢ÐµÐºÑÑ‚: {text_preview}")
                print()
                
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð¾Ð¸ÑÐºÐ°: {e}")
    
    # Ð˜Ð½Ñ‚ÐµÑ€Ð°ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº
    print("\nðŸŽ® Ð˜ÐÐ¢Ð•Ð ÐÐšÐ¢Ð˜Ð’ÐÐ«Ð™ ÐŸÐžÐ˜Ð¡Ðš")
    print("=" * 60)
    print("Ð’Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð²Ð°Ñˆ Ð·Ð°Ð¿Ñ€Ð¾Ñ (Ð¸Ð»Ð¸ 'quit' Ð´Ð»Ñ Ð²Ñ‹Ñ…Ð¾Ð´Ð°):")
    
    while True:
        try:
            user_query = input("\nðŸ” Ð—Ð°Ð¿Ñ€Ð¾Ñ: ").strip()
            
            if user_query.lower() in ['quit', 'exit', 'Ð²Ñ‹Ñ…Ð¾Ð´']:
                break
            
            if not user_query:
                continue
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚
            context = get_context_for_query(user_query, vector_store, conn)
            
            if context:
                print(f"\nðŸ“„ ÐÐ°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ ({len(context)} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²):")
                print("-" * 40)
                print(context[:500] + "..." if len(context) > 500 else context)
            else:
                print("âŒ ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½")
                
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")
    
    # Ð—Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ðµ
    conn.close()
    print("\nðŸ‘‹ Ð¢ÐµÑÑ‚ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½!")

def show_database_stats():
    """
    ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÑƒ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…
    """
    print("\nðŸ“Š Ð¡Ð¢ÐÐ¢Ð˜Ð¡Ð¢Ð˜ÐšÐ Ð‘ÐÐ—Ð« Ð”ÐÐÐÐ«Ð¥")
    print("=" * 60)
    
    db_path = os.path.join(os.getcwd(), 'knowledge_base_v2.db')
    
    if not os.path.exists(db_path):
        print("âŒ Ð‘Ð°Ð·Ð° Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°")
        return
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        # ÐžÐ±Ñ‰Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
        cursor.execute("SELECT COUNT(*) FROM chunks")
        total_chunks = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(DISTINCT document_id) FROM chunks")
        unique_docs = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(DISTINCT doc_type_short) FROM chunks WHERE doc_type_short != ''")
        unique_types = cursor.fetchone()[0]
        
        print(f"ðŸ“„ Ð’ÑÐµÐ³Ð¾ Ñ‡Ð°Ð½ÐºÐ¾Ð²: {total_chunks}")
        print(f"ðŸ“š Ð£Ð½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ñ… Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {unique_docs}")
        print(f"ðŸ·ï¸  Ð¢Ð¸Ð¿Ð¾Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²: {unique_types}")
        
        # Ð¢Ð¾Ð¿ Ñ‚Ð¸Ð¿Ð¾Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²
        cursor.execute("""
            SELECT doc_type_short, COUNT(*) as count 
            FROM chunks 
            WHERE doc_type_short != '' 
            GROUP BY doc_type_short 
            ORDER BY count DESC 
            LIMIT 5
        """)
        
        print("\nðŸ† Ð¢Ð¾Ð¿-5 Ñ‚Ð¸Ð¿Ð¾Ð² Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²:")
        for doc_type, count in cursor.fetchall():
            print(f"  â€¢ {doc_type}: {count} Ñ‡Ð°Ð½ÐºÐ¾Ð²")
        
        # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ Ð´Ð»Ð¸Ð½Ðµ Ñ‚ÐµÐºÑÑ‚Ð°
        cursor.execute("SELECT AVG(LENGTH(chunk_text)), MIN(LENGTH(chunk_text)), MAX(LENGTH(chunk_text)) FROM chunks")
        avg_len, min_len, max_len = cursor.fetchone()
        
        print(f"\nðŸ“ Ð”Ð»Ð¸Ð½Ð° Ñ‚ÐµÐºÑÑ‚Ð¾Ð²:")
        print(f"  â€¢ Ð¡Ñ€ÐµÐ´Ð½ÑÑ: {avg_len:.0f} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
        print(f"  â€¢ ÐœÐ¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ: {min_len} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
        print(f"  â€¢ ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð°Ñ: {max_len} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²")
        
        conn.close()
        
    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ°: {e}")

if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == "--stats":
        show_database_stats()
    else:
        test_vector_search() 