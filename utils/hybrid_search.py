"""
–ú–æ–¥—É–ª—å –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞, –æ–±—ä–µ–¥–∏–Ω—è—é—â–µ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏ FTS –ø–æ–∏—Å–∫.

–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –æ–±–µ–∏—Ö —Å–∏—Å—Ç–µ–º –ø–æ–∏—Å–∫–∞
–∏ –∏—Ö –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤ –µ–¥–∏–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
"""
import sqlite3
import logging
from typing import List, Dict, Any, Optional
import os
from config import DEFAULT_TOP_K, MAX_CONTEXT_TOKENS

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –º–æ–¥—É–ª–µ–π
from utils.vector_search import search_relevant_chunks, get_chunks_by_ids
from utils.fts_search import fts_search, merge_results

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–≥–µ—Ä –¥–ª—è –º–æ–¥—É–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
hybrid_logger = logging.getLogger('hybrid_search')

# –ü—É—Ç—å –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
DB_PATH = os.path.join(os.getcwd(), 'knowledge_base_v2.db')

def get_hybrid_context(query: str, vector_store, conn: sqlite3.Connection, top_k: int = DEFAULT_TOP_K, max_tokens: int = MAX_CONTEXT_TOKENS) -> str:
    """
    –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç, –æ–±—ä–µ–¥–∏–Ω—è—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏ FTS –ø–æ–∏—Å–∫–∞.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        vector_store: –≠–∫–∑–µ–º–ø–ª—è—Ä FAISSVectorStore –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        conn: –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö SQLite
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞
        max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
    Returns:
        –°—Ç—Ä–æ–∫–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º, —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    """
    hybrid_logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
    vec_results = []
    fts_results = []
    
    # 1. –í—ã–ø–æ–ª–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (–µ—Å–ª–∏ vector_store –¥–æ—Å—Ç—É–ø–µ–Ω)
    if vector_store:
        try:
            vec_results = search_relevant_chunks(query, vector_store, conn, top_k)
            hybrid_logger.info(f"–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª {len(vec_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        except Exception as e:
            hybrid_logger.error(f"–û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {str(e)}")
    else:
        hybrid_logger.warning("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ FTS –ø–æ–∏—Å–∫")
    
    # 2. –í—ã–ø–æ–ª–Ω—è–µ–º FTS –ø–æ–∏—Å–∫
    try:
        fts_results = fts_search(query, limit=top_k)
        hybrid_logger.info(f"FTS –ø–æ–∏—Å–∫ –Ω–∞—à–µ–ª {len(fts_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    except Exception as e:
        hybrid_logger.error(f"–û—à–∏–±–∫–∞ FTS –ø–æ–∏—Å–∫–∞: {str(e)}")
    
    # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    merged_results = merge_results(vec_results, fts_results)
    hybrid_logger.info(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(merged_results)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    # 4. –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
    if not merged_results:
        hybrid_logger.warning("–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ –Ω–∞—à–µ–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return ""
    
    context_parts = []
    
    for chunk in merged_results:
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ –∏ —Ç–∏–ø–µ –ø–æ–∏—Å–∫–∞
        search_type = chunk.get('search_type', 'unknown')
        search_type_label = 'üîç –í–µ–∫—Ç–æ—Ä–Ω—ã–π' if search_type == 'vector' else 'üìù –¢–µ–∫—Å—Ç–æ–≤—ã–π'
        
        document_info = f"{search_type_label} –ø–æ–∏—Å–∫ - –î–æ–∫—É–º–µ–Ω—Ç: {chunk.get('doc_id', chunk.get('document_title', chunk.get('document_id', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç')))}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —á–∞–Ω–∫–∞
        content = chunk.get('chunk_text', chunk.get('text', chunk.get('content', '')))
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        similarity = chunk.get('similarity', 0)
        similarity_info = f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {similarity:.3f}"
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —á–∞–Ω–∫–∞
        chunk_text = f"{document_info} ({similarity_info})\n\n{content}\n\n"
        context_parts.append(chunk_text)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    context = "--- –ö–û–ù–¢–ï–ö–°–¢ –ò–ó –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô (–ì–ò–ë–†–ò–î–ù–´–ô –ü–û–ò–°–ö) ---\n\n" + "\n".join(context_parts)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ –¥–ª–∏–Ω–µ (–ø—Ä–∏–º–µ—Ä–Ω–æ)
    if len(context) > max_tokens * 4:  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –∫ —Å–∏–º–≤–æ–ª–∞–º
        context = context[:max_tokens * 4] + "...\n\n--- –ö–û–ù–ï–¶ –ö–û–ù–¢–ï–ö–°–¢–ê ---"
    else:
        context += "\n\n--- –ö–û–ù–ï–¶ –ö–û–ù–¢–ï–ö–°–¢–ê ---"
    
    hybrid_logger.info(f"–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª–∏–Ω–æ–π {len(context)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    return context

def hybrid_search(query: str, vector_store, conn: sqlite3.Connection, top_k: int = DEFAULT_TOP_K) -> List[Dict[str, Any]]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫, –≤–æ–∑–≤—Ä–∞—â–∞—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        vector_store: –≠–∫–∑–µ–º–ø–ª—è—Ä FAISSVectorStore –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        conn: –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö SQLite
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–∞—Ö
    """
    hybrid_logger.info(f"–í—ã–ø–æ–ª–Ω—è–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{query}'")
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
    vec_results = []
    fts_results = []
    
    # 1. –í—ã–ø–æ–ª–Ω—è–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
    if vector_store:
        try:
            vec_results = search_relevant_chunks(query, vector_store, conn, top_k)
            hybrid_logger.info(f"–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫: {len(vec_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        except Exception as e:
            hybrid_logger.error(f"–û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: {str(e)}")
    
    # 2. –í—ã–ø–æ–ª–Ω—è–µ–º FTS –ø–æ–∏—Å–∫
    try:
        fts_results = fts_search(query, limit=top_k)
        hybrid_logger.info(f"FTS –ø–æ–∏—Å–∫: {len(fts_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    except Exception as e:
        hybrid_logger.error(f"–û—à–∏–±–∫–∞ FTS –ø–æ–∏—Å–∫–∞: {str(e)}")
    
    # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    merged_results = merge_results(vec_results, fts_results)
    hybrid_logger.info(f"–ò—Ç–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(merged_results)}")
    
    return merged_results

def get_search_statistics(query: str, vector_store, conn: sqlite3.Connection, top_k: int = DEFAULT_TOP_K) -> Dict[str, Any]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞.
    
    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        vector_store: –≠–∫–∑–µ–º–ø–ª—è—Ä FAISSVectorStore –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
        conn: –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö SQLite
        top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –ø–æ–∏—Å–∫–∞
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π –ø–æ–∏—Å–∫–∞
    """
    stats = {
        'query': query,
        'vector_results': 0,
        'fts_results': 0,
        'merged_results': 0,
        'overlap_count': 0,
        'vector_available': vector_store is not None,
        'fts_available': False
    }
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å FTS
        cursor = conn.cursor()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='chunks_fts'")
        stats['fts_available'] = cursor.fetchone() is not None
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        vec_results = []
        fts_results = []
        
        if vector_store:
            try:
                vec_results = search_relevant_chunks(query, vector_store, conn, top_k)
                stats['vector_results'] = len(vec_results)
            except Exception as e:
                hybrid_logger.error(f"–û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ: {str(e)}")
        
        if stats['fts_available']:
            try:
                fts_results = fts_search(query, limit=top_k)
                stats['fts_results'] = len(fts_results)
            except Exception as e:
                hybrid_logger.error(f"–û—à–∏–±–∫–∞ FTS –ø–æ–∏—Å–∫–∞ –≤ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–µ: {str(e)}")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
        if vec_results and fts_results:
            vec_ids = set(r.get('chunk_id', r.get('id')) for r in vec_results)
            fts_ids = set(r.get('chunk_id', r.get('id')) for r in fts_results)
            stats['overlap_count'] = len(vec_ids & fts_ids)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        merged_results = merge_results(vec_results, fts_results)
        stats['merged_results'] = len(merged_results)
        
    except Exception as e:
        hybrid_logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {str(e)}")
    
    return stats

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    import sqlite3
    from utils.vector_search import FAISSVectorStore
    from config import EMBEDDING_DIMENSION
    
    print("üîÑ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ì–ò–ë–†–ò–î–ù–û–ì–û –ü–û–ò–°–ö–ê")
    print("=" * 60)
    
    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    db_path = os.path.join(os.getcwd(), 'knowledge_base_v2.db')
    
    if not os.path.exists(db_path):
        print("‚ùå –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        exit()
    
    conn = sqlite3.connect(db_path)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    vector_store = None
    try:
        vector_store = FAISSVectorStore(embedding_dimension=EMBEDDING_DIMENSION)
        if vector_store.load_index():
            print("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ")
        else:
            print("‚ö†Ô∏è –í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
            vector_store = None
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {e}")
        vector_store = None
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = ["–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞", "–∞–Ω–∞–ª–∏–∑ –∫—Ä–æ–≤–∏", "–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ü–∏–µ–Ω—Ç–æ–≤"]
    
    for query in test_queries:
        print(f"\nüîç –ó–∞–ø—Ä–æ—Å: '{query}'")
        print("-" * 50)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = get_search_statistics(query, vector_store, conn)
        print(f"üìä –í–µ–∫—Ç–æ—Ä–Ω—ã–π: {stats['vector_results']}, FTS: {stats['fts_results']}, –ò—Ç–æ–≥–æ: {stats['merged_results']}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
        results = hybrid_search(query, vector_store, conn, top_k=3)
        
        for i, result in enumerate(results[:3], 1):
            search_type = result.get('search_type', 'unknown')
            doc_id = result.get('document_id', 'N/A')
            similarity = result.get('similarity', 0)
            
            print(f"   {i}. [{search_type.upper()}] {doc_id} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {similarity:.3f})")
    
    conn.close()
    print("\n‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ") 